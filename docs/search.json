[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Computing and Communication",
    "section": "",
    "text": "Welcome\nThis is the website for my notes on Statistical Computing and Communication, which are currently a work in progress.\n\n\n\n\n\n\nWarning\n\n\n\nUnder construction."
  },
  {
    "objectID": "guiding-principles.html#reproducibility",
    "href": "guiding-principles.html#reproducibility",
    "title": "1  Guiding principles",
    "section": "1.1 Reproducibility",
    "text": "1.1 Reproducibility"
  },
  {
    "objectID": "guiding-principles.html#organization",
    "href": "guiding-principles.html#organization",
    "title": "1  Guiding principles",
    "section": "1.2 Organization",
    "text": "1.2 Organization"
  },
  {
    "objectID": "guiding-principles.html#automation",
    "href": "guiding-principles.html#automation",
    "title": "1  Guiding principles",
    "section": "1.3 Automation",
    "text": "1.3 Automation"
  },
  {
    "objectID": "guiding-principles.html#portability",
    "href": "guiding-principles.html#portability",
    "title": "1  Guiding principles",
    "section": "1.4 Portability",
    "text": "1.4 Portability"
  },
  {
    "objectID": "guiding-principles.html#synchronization",
    "href": "guiding-principles.html#synchronization",
    "title": "1  Guiding principles",
    "section": "1.5 Synchronization",
    "text": "1.5 Synchronization"
  },
  {
    "objectID": "guiding-principles.html#scalability",
    "href": "guiding-principles.html#scalability",
    "title": "1  Guiding principles",
    "section": "1.6 Scalability",
    "text": "1.6 Scalability"
  },
  {
    "objectID": "fundamental-skills.html",
    "href": "fundamental-skills.html",
    "title": "Fundamental Skills",
    "section": "",
    "text": "The following chapters cover a set of broadly useful fundamental skills."
  },
  {
    "objectID": "r-programming-basics.html",
    "href": "r-programming-basics.html",
    "title": "2  R programming",
    "section": "",
    "text": "See Chapters 2-8 of Advanced R."
  },
  {
    "objectID": "tidyverse.html",
    "href": "tidyverse.html",
    "title": "3  Tidyverse",
    "section": "",
    "text": "See Chapters 1-20 of R for Data Science."
  },
  {
    "objectID": "linux-basics.html",
    "href": "linux-basics.html",
    "title": "4  Linux basics",
    "section": "",
    "text": "For a first, brief introduction to Linux, read this page and then this page. For a somewhat more systematic introduction to the Linux shell, read Part I of The Linux Command Line: A Complete Introduction (Penn link). To build your Linux skills further, read Parts II and III as well."
  },
  {
    "objectID": "version-control.html",
    "href": "version-control.html",
    "title": "5  Version control",
    "section": "",
    "text": "Read Chapters 1 and 2 of Pro Git."
  },
  {
    "objectID": "directories.html",
    "href": "directories.html",
    "title": "6  Directories",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "real-data.html",
    "href": "real-data.html",
    "title": "7  Real data",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "r-programming.html",
    "href": "r-programming.html",
    "title": "R programming",
    "section": "",
    "text": "When programming, one often wishes to get to the answer as quickly as possible. However, programming quickly jeopardizes the correctness and reproducibility of the code that is written. The following chapters lay out a number of programming best practices to promote the correctness and reproducibility of code, which may take additional time to master at first but which save time in the long run. This document is geared towards R programming, though many of the principles presented here are language-agnostic.\nChapter 8 is for general suggestions, whereas Chapters 9 and 10 are specific to R scripting and R packages, respectively."
  },
  {
    "objectID": "programming-best-practices.html#code-readability",
    "href": "programming-best-practices.html#code-readability",
    "title": "8  Best practices",
    "section": "8.1 Code readability",
    "text": "8.1 Code readability\nCode readability refers to how easily a programmer can understand and interpret a piece of code. It is an essential aspect of software development because readable code is easier to maintain, faster to debug, more collaborative, and less prone to errors. Code readability can be broadly categorized into two main aspects: code style and code transparency. Code style pertains primarily to the format and presentation of the code. Code transparency delves deeper into the logic and structure of the code. It refers to how straightforwardly the functionality, logic, or operations are conveyed.\n\n8.1.1 Example\nConsider the following two examples:\nPoor code readability:\n\na = sum(dat[dat[,1]&gt;5 & dat[,2]&lt;10,3])\nb = mean(dat[dat[,1]&gt;5 & dat[,2]&lt;10,3])\nprint(paste('Sum =',a,', Mean =',b))\n\nGood code readability:\n\n# Library for data manipulation\nlibrary(dplyr)\n\n# Analysis parameters\nMIN_AGE &lt;- 5\nMAX_SCORE &lt;- 10\n\n# Extract summary performance metrics for subset of observations\nsummary_data &lt;- dat |&gt;\n  filter(age &gt;= MIN_AGE & score &lt;= MAX_SCORE) |&gt;\n  summarize(\n    sum_performance = sum(performance_metric),\n    mean_performance = mean(performance_metric)\n  )\n\n# Print the computed results\ncat(sprintf(\"Sum of Performance: %f | Mean of Performance: %f\", \n            summary_data$sum_performance, \n            summary_data$mean_performance))\n\nIn this section, we will discuss how to write code like that in the second snippet above.\n\n\n8.1.2 Code style\nCode should adhere to the tidyverse style guide, which includes guidance on the following aspects of code:\n\nNaming conventions\nSpacing and indentation\nCommenting\n\nYou can use the styler package to automatically conform your spacing and indentation to the tidyverse style guide."
  },
  {
    "objectID": "programming-best-practices.html#code-transparency",
    "href": "programming-best-practices.html#code-transparency",
    "title": "8  Best practices",
    "section": "8.2 Code transparency",
    "text": "8.2 Code transparency\nTo write transparent code, follow these guidelines:\n\nUse tidyverse paradigms as much as possible (e.g. dplyr summaries instead of apply operations)\nUse names, rather than indices, for subsetting (e.g. results[\"mse\", \"lasso\"] versus results[2,4])\nUse named arguments in function calls, especially with more than one argument (e.g. rbinom(3, 1, 0.5) versus rbinom(n = 3, size = 1, prob = 0.5))\nPut logically related chunks of code together into code blocks, with a comment describing the thrust of that code block.\nName constants in your scripts. “Magic numbers” are unexplained numbers in your scripts:\n\n# Bad practice: Magic number\nif (x &gt; 30) ...\n\n# Good practice: Using a named constant\nMAX_AGE &lt;- 30\nif (x &gt; MAX_AGE) ...\n\nDescriptive constant names provide clarity. Furthermore, especially if these constants are used in multiple places throughout your script, updating them becomes as simple as changing one line of code. It is also advisable to put all such constants together, near the top of the script."
  },
  {
    "objectID": "programming-best-practices.html#modularity",
    "href": "programming-best-practices.html#modularity",
    "title": "8  Best practices",
    "section": "8.3 Modularity",
    "text": "8.3 Modularity\nAvoid repetitive code. Repetition not only lengthens your script but also increases the chance for mistakes.\n\n# Bad practice: Repetitive code\ndata$age[data$age &lt; 0] &lt;- NA\ndata$score[data$score &lt; 0] &lt;- NA\n\n# Good practice: Create a function\nreplace_negatives_with_NA &lt;- function(variable) {\n  variable[variable &lt; 0] &lt;- NA\n  return(variable)\n}\n\ndata$age &lt;- replace_negatives_with_NA(data$age)\ndata$score &lt;- replace_negatives_with_NA(data$score)"
  },
  {
    "objectID": "programming-best-practices.html#code-speed",
    "href": "programming-best-practices.html#code-speed",
    "title": "8  Best practices",
    "section": "8.4 Code speed",
    "text": "8.4 Code speed\nWhen optimizing the execution speed of your code, it’s essential to strike a balance between code readability and efficiency. However, as Donald Knuth famously stated, ‘premature optimization is the root of all evil.’ Focus on writing clean and functional code first. Once your code works correctly, you can then consider optimizing the most computationally intensive parts if necessary.\n\n8.4.1 Vectorization\nR is a vectorized language, which means that operations can be performed on entire vectors rather than looping over individual elements. For example, instead of using a loop to square each element of a vector, you can simply square the vector directly:\n\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Non-vectorized operation using a loop\nsquared_numbers &lt;- vector(\"numeric\", length(numbers))\nfor (i in seq_along(numbers)) {\n  squared_numbers[i] &lt;- numbers[i]^2\n}\n\n# Example of vectorized operation\nsquared_numbers &lt;- numbers^2\n\n\n\n8.4.2 Factoring code out of loops\nOften, parts of the code inside a loop don’t depend on the loop variable and can be taken outside the loop, leading to efficiency gains. For example, if you’re repeatedly computing something within a loop that doesn’t change, compute it once outside the loop.\n\n# Inefficient loop: Compute mean of the entire dataset in each iteration\nfor (i in 1:n_bootstrap) {\n  sample &lt;- sample(data_points, size = length(data_points), replace = TRUE)\n  mu_data &lt;- mean(data_points) # This is unnecessary in the loop\n  bootstrap_means[i] &lt;- mean(sample) - mu_data\n}\n\n# Optimized loop: Factor out the mean computation of the dataset\nbootstrap_means_optimized &lt;- numeric(n_bootstrap)\nmu_data &lt;- mean(data_points)\nfor (i in 1:n_bootstrap) {\n  sample &lt;- sample(data_points, size = length(data_points), replace = TRUE)\n  bootstrap_means[i] &lt;- mean(sample) - mu_data\n}"
  },
  {
    "objectID": "r-scripting.html#managing-package-dependencies",
    "href": "r-scripting.html#managing-package-dependencies",
    "title": "9  Writing R scripts",
    "section": "9.1 Managing package dependencies",
    "text": "9.1 Managing package dependencies\nR’s rich ecosystem of packages is one of its strengths. However, with the increasing number of packages, managing dependencies becomes essential to ensure the reproducibility and reliability of your code. This section provides some best practices and recommended tools for handling package dependencies.\n\n9.1.1 Namespace conflicts\nWhen you load multiple packages, there is potential for functions with the same name to clash, leading to ambiguity in your code. This is because different packages can have functions that share the same name.\n\nUsing :: for explicit namespaces\nOne way to deal with such conflicts is by explicitly specifying the package’s namespace when calling the function using the :: syntax. This ensures that the correct function from the desired package is called. For example, if both packages A and B have a function named fun(), and you want to use the function from package A, you can do:\n\nA::fun()\n\n\n\nconflicted package\nAnother solution is to use the conflicted package. When loaded, conflicted will prevent you from using any function that has a namespace conflict, prompting you to specify which one you want:\n\nlibrary(conflicted)\nconflict_prefer(\"fun\", \"A\") # Always use fun from package A\n\n\n\n\n9.1.2 Managing package versions with renv\nFor the sake of reproducibility and easy collaboration, it is essential to track which versions of R packages were used for a given analysis. The renv package can help by creating isolated, reproducible R environments. Using renv, you can snapshot your current package versions, ensuring that others (or yourself in the future) can replicate your environment:\n\n# initializing renv\nrenv::init()\n\nBy doing this, renv will create a snapshot of your current package versions and save it in a renv.lock file. This file can then be shared, ensuring that the same package versions are used when your code is run elsewhere."
  },
  {
    "objectID": "r-scripting.html#code-portability",
    "href": "r-scripting.html#code-portability",
    "title": "9  Writing R scripts",
    "section": "9.2 Code portability",
    "text": "9.2 Code portability\nOne of the challenges in R programming, especially when sharing your code with others or transferring it between different machines, is ensuring code portability. Code portability ensures that your R script or project can be executed seamlessly in different environments without any hitches. Here’s how to enhance the portability of your R code:\n\n9.2.1 Avoid hardcoded paths\nHardcoding paths, such as C:/Users/JohnDoe/Documents/mydata.csv, can break the code when run on a different machine or if files are moved. Instead, always aim to use relative paths or dynamic paths that adjust based on the current directory. This ensures that as long as the directory structure remains consistent, the paths will always be correct.\n\n\n9.2.2 Use R Projects\nR Projects, a feature of RStudio, allow you to maintain a consistent working directory regardless of where the project is located on the machine. When you open an R Project, RStudio sets the working directory to the project’s root directory. This allows you to use relative paths effectively. To start an R Project, simply choose File &gt; New Project in RStudio."
  },
  {
    "objectID": "r-scripting.html#automation-and-reproducibility",
    "href": "r-scripting.html#automation-and-reproducibility",
    "title": "9  Writing R scripts",
    "section": "9.3 Automation and reproducibility",
    "text": "9.3 Automation and reproducibility\nTo ensure the robustness and reliability of your analyses, strive for automation and reproducibility. This approach ensures your work remains consistent and easily repeatable.\n\n9.3.1 Automate manual operations\nManual operations, including moving files, creating directories, and saving figures, are inefficient and error-prone. As many operations as possible should be automated instead.\n\n# Instead of manually downloading a file, use R to do it programmatically:\ndownload.file(url = \"https://example.com/data.csv\", destfile = \"data/data.csv\")\n\n\n# Instead of manually saving figures, use ggsave():\nggsave(\"plots/my_plot.png\", plot = p)\n\n\n\n9.3.2 Produce results by running entire scripts\nAll results should be produced by writing scripts and then executing those scripts in their entirety. While pieces of the script can be run manually during development, run the entire script on the data when the time comes to actually produce a result.\n\n\n9.3.3 Best practices\nLastly, always load required libraries at the top of your R scripts. This not only makes it clear which packages are necessary but also ensures that potential namespace conflicts are identified early on, leading to more predictable and stable code."
  },
  {
    "objectID": "r-packages.html#defensive-programming",
    "href": "r-packages.html#defensive-programming",
    "title": "10  Writing R packages",
    "section": "10.1 Defensive programming",
    "text": "10.1 Defensive programming\nWhen writing a function, the goal is for the function to either produce the intended output or an informative error message for any given input. Since a wide variety of inputs are possible, defensive programming is required to achieve this. Defensive programming in R entails anticipating potential issues, pitfalls, or mistakes in the code and implementing strategies to handle them gracefully.\nInput validation:\nOne of the most common tactics in defensive programming is input validation. Before processing, check if the provided inputs are valid or in the expected format. For instance, if a function expects a numeric vector, verify the input type before proceeding.\n\ncompute_mean &lt;- function(data_vector) {\n  if (!is.numeric(data_vector)) {\n    stop(\"The input data_vector must be numeric.\")\n  }\n  return(mean(data_vector))\n}\n\nUse informative error messages:\nWhen an error condition is detected, provide a clear and informative error message. This not only prevents silent failures but also helps users or developers identify and fix the problem.\n\nsafe_divide &lt;- function(numerator, denominator) {\n  if (denominator == 0) {\n    stop(\"Error: Division by zero is not allowed.\")\n  }\n  return(numerator / denominator)\n}"
  },
  {
    "objectID": "r-packages.html#further-reading",
    "href": "r-packages.html#further-reading",
    "title": "10  Writing R packages",
    "section": "10.2 Further reading",
    "text": "10.2 Further reading\nSee R Packages by Hadley Wickham and Jenny Bryan."
  },
  {
    "objectID": "hpc.html",
    "href": "hpc.html",
    "title": "High-performance computing",
    "section": "",
    "text": "High-performance computing is a computing paradigm that facilitates parallel computation across hundreds of computers. It is particularly effective for embarrassingly parallelizable computations, i.e. computations with many subparts that can be run independently of one another. Many institutions (academic and otherwise) have some sort of high-performance computing environment available for its members to use."
  },
  {
    "objectID": "hpc-basics.html#the-anatomy-of-an-hpc-cluster",
    "href": "hpc-basics.html#the-anatomy-of-an-hpc-cluster",
    "title": "11  High-performance computing basics",
    "section": "11.1 The anatomy of an HPC cluster",
    "text": "11.1 The anatomy of an HPC cluster\n\nHardware\n\nOverview\nA high-performance computer cluster is a large collection of computers connected by a network to each other and to a common disk storage equipped with a parallel distributed file system (Figure 11.1). Each computer is called a node, and these nodes are divided into two groups: login nodes and compute nodes. Login nodes are like a lobby area for the HPC; they are where you end up when you first enter. There are usually just a few login nodes. The vast majority of the nodes in an HPC are compute nodes; they are the workhorses of the HPC. Each compute node has some number of cores (e.g. 16) as well as some amount of memory shared among those cores (e.g. 128 GB).\n\n\n\nFigure 11.1: High-performance computing architecture\n\n\n\n\nComponents\n\nDisk storage: Disk storage refers to devices that hold vast amounts of data persistently, even when power is switched off. Disk storage houses the file system of the HPC cluster.\nParallel distributed file system: A parallel distributed filesystem is designed to store and manage data across a multitude of machines concurrently, allowing multiple computers to read and write data in parallel.\nNetwork: A network is a system of high-speed connections linking together computational nodes and storage systems.\nNode: A node is a single computational unit within a cluster. Each node contains one or more CPUs and memory. An HPC cluster can consist of a few to thousands of nodes, working together to execute large-scale computational tasks.\nCPU (Central Processing Unit): The CPU is the primary computational engine of an HPC node. It executes the instructions of a computer program and interacts with other system components. CPUs typically have multiple cores, allowing them to process several tasks simultaneously.\nCore: A core is an individual processing unit within a CPU. Each core can independently execute instructions, so computational parallelization on HPC clusters typically occurs at the core level.\nMemory/RAM (Random-Access Memory): RAM is the primary storage medium that the CPU uses to store and retrieve data during computations. It offers fast read and write access. Each node has its own RAM, which is shared among the cores on that node.\n\n\n\n\nSoftware\nTypically, HPC clusters come with several version of a number of standard softwares pre-installed for all users. Users can build up their computing environment by loading modules corresponding to specific versions of specific softwares (e.g. R/4.3.1). The command module avail lists all available modules that can be loaded by the user, while module load &lt;module_name&gt; loads the specified module. The module system provides a convenient way to dynamically modify a user’s environment to include the necessary executables, libraries, and other resources required by a software package. This approach allows multiple versions of software to coexist on the cluster and enables users to switch between them as needed, facilitating reproducibility and flexibility in using various software tools and libraries on HPC clusters."
  },
  {
    "objectID": "hpc-basics.html#parallel-computing-paradigms",
    "href": "hpc-basics.html#parallel-computing-paradigms",
    "title": "11  High-performance computing basics",
    "section": "11.2 Parallel computing paradigms",
    "text": "11.2 Parallel computing paradigms\nParallel processing on HPC clusters allows multiple tasks to be carried out simultaneously. In this context, a “task” is a unit of computation. These tasks can be independent pieces of work, stages in a larger operation, or simultaneous computations on different data subsets. There are several parallel computing paradigms that differ based on how tasks communicate and share data. The main such paradigms are listed below, roughly in increasing order of complexity.\n\nEmbarrassingly parallel paradigm\nThis paradigm represents a collection of tasks where each task is entirely independent and does not need to communicate with other tasks. Because no inter-task communication or synchronization is required, the embarrassingly parallel paradigm is the simplest parallel computing paradigm. Common examples include parameter sweeps and Monte Carlo simulations.\n\n\nShared memory and graphics processing unit (GPU) parallelism\nShared memory parallelism refers to a parallel computing paradigm where multiple cores within a single node access and operate on a single, shared memory space. Shared memory parallelism can leverage multiple cores on a CPU or GPU (graphics processing unit). GPUs are a different kind of processor designed to perform repetitive matrix operations on large datasets efficiently, making them especially adept at tasks like image processing and deep learning Some HPC clusters have a number of GPU nodes in addition to the standard CPU nodes. Programming shared memory parallelism on CPUs usually involves the OpenMP application programming interface, while programming GPUs usually involves the CUDA programming language. Both approaches require careful management of memory hierarchies, thread synchronization, and data access patterns.\n\n\nDistributed memory parallelism\nIn this paradigm, tasks run on cores of separate nodes, each equipped with its own local memory. The primary mode of communication is passing messages across the network between nodes, often facilitated by protocols like the Message Passing Interface (MPI). Distributed memory parallelism requires careful management of data distribution and communication patterns to avoid bottlenecks and ensure efficient parallelization. Common applications of distributed memory parallelism involve big data processing and graph processing."
  },
  {
    "objectID": "hpc-basics.html#hpc-resource-requests-and-job-scheduling",
    "href": "hpc-basics.html#hpc-resource-requests-and-job-scheduling",
    "title": "11  High-performance computing basics",
    "section": "11.3 HPC resource requests and job scheduling",
    "text": "11.3 HPC resource requests and job scheduling\nHPC resources are shared among many users, so each user must request the resources they need for their computations. HPC computations are structured into jobs, which are submitted to a software called a scheduler that manages the allocation of resources to each job.\n\nTwo kinds of jobs\nThere are two kinds of jobs on an HPC cluster: interactive jobs and batch jobs.\n\nAn interactive job entails directly entering a compute node and running computer programs from there. (It may be tempting to start computing directly in a login node, but this is strongly discouraged.) Interactive jobs are useful for initial exploration and debugging but not for more serious computations. Indeed, interactive jobs do not give you access to multiple compute nodes and whatever computations you start will be interrupted if you lose connection with the cluster.\nA batch job is a computing job that you “send to the cluster” for completion. Instead of issuing commands manually as in an interactive job, a batch job entails a sequence of commands specified in a submission script. A batch job may involve one or more tasks. For example, an array job is a batch job that consists of multiple array tasks. Array jobs are a typical means to carry out embarrassingly parallel computations. Unlike interactive jobs, batch jobs like array jobs can take advantage of the cluster’s multiple compute nodes. Furthermore, once you submit a batch job, you need not be logged into the cluster for the job to finish running.\n\n\n\nResource requests\nWhether you would like to run an interactive job or a batch job, you need to request resources from the scheduler. Below are the kinds of resources that are typically requested for interactive and batch jobs.\n\nTime (per job or per task): The amount of time you expect your job (or each constituent task) to run, specified in hours, minutes, and seconds.\nMemory (per job or per task): The amount of memory you expect your job (or each constituent task) to use, often specified in gigabytes (GB).\nNumber of cores (per job or per task): Array jobs often will have multiple tasks, but with just one core per task. Other kinds of batch jobs may have multiple cores per task.\n\nEach kind of resource typically has a default setting that is used if you do not specify a value. For example, typically one core will be requested per job or per task by default.\nImportantly, jobs or tasks that exceed their requested time or memory limits will be terminated. Thus, it is important to request enough time and memory for your job to complete successfully. However, requesting too much time or memory can be problematic as well, as it may delay the start of your job. Thus, a good strategy is to run a small test job to get a sense of how much time and memory your computations require, add a little bit of wiggle room, and then request that amount of time and memory for your actual job. For example, if your test job takes 10 minutes and uses 1 GB of memory, you might request 15 minutes and 2 GB of memory for your actual job.\n\n\nJob scheduling\nThe scheduler manages the allocation of resources to each job. It does so by maintaining a queue of jobs waiting to be executed. When a job is submitted, it is placed in the queue. The scheduler then allocates resources to each job in the queue based on their resource requests and the order in which they are submitted. Below is an example of how a scheduler may allocate compute and memory resources across time to several jobs.\n\n\n\nScheduling multiple jobs with different resource requirements\n\n\nThe scheduler may also impose limits on the amount of resources that can be requested by a single user at a given time. For example, a scheduler may limit the number of cores that a user can request at a given time to 100. This limit is called a fair-share limit. Many HPC clusters have multiple queues for different kinds of jobs. For example, there may be separate queues for jobs with particularly short or long requested runtimes, or for jobs with particularly small or large requested memory allocations.\n\n\nScheduler commands\nThere are several different schedulers in use by HPC clusters, two of the most common being Sun Grid Engine (SGE) and Simple Linux Utility for Resource Management (SLURM). These different schedulers have different syntax for tasks like submitting jobs, viewing the job queue, etc.\n\n\n\n\n\n\n\n\nTask\nSGE Command\nSLURM Command\n\n\n\n\nSubmit interactive job\nqrsh\nsrun\n\n\nSubmit batch job\nqsub &lt;script&gt;\nsbatch &lt;script&gt;\n\n\nResource request flags\n-l h_rt=01:00:00 -l m_mem_free=1G\n--time=01:00:00 --mem=1G\n\n\nDelete a job\nqdel &lt;job_id&gt;\nscancel &lt;job_id&gt;\n\n\nView job status\nqstat\nsqueue\n\n\nJob accounting\nqacct\nsacct\n\n\n\nNote that there is heterogeneity within the syntax used even within SGE or SLURM schedulers; please check your specific cluster’s documentation."
  },
  {
    "objectID": "hpc-basics.html#interacting-with-hpc-clusters",
    "href": "hpc-basics.html#interacting-with-hpc-clusters",
    "title": "11  High-performance computing basics",
    "section": "11.4 Interacting with HPC clusters",
    "text": "11.4 Interacting with HPC clusters\nTypically, the HPC workflow involves the following steps:\n\nDevelop and debug code locally (i.e., on your laptop or desktop computer).\nMove your code (and data, if applicable) onto the cluster.\nTest-run your code on the cluster via an interactive job.\nSubmit a batch job to carry out your parallel computations.\nCombine the results from individual jobs into a single file.\nMove the results onto your local machine.\nExplore the results on your local machine.\n\nThe following sections detail some of the steps described above.\n\nLogging onto the cluster\nLogging onto a cluster can be done either via the Terminal or via a remote desktop. I will focus on the former approach. To log into a cluster via the Terminal, one typically uses ssh, e.g.\nssh ekatsevi@hpc3.wharton.upenn.edu\nUsually, you will be required to enter a password at this stage. However, I recommend setting up SSH keys in order to avoid entering your password each time you log onto the cluster. Upon gaining access, you will land in a login node.\n\n\nMoving code and data between your computer and the cluster\n\nCode\nThe best way to move code between your local computer and the cluster is via GitHub. For example, to get code from your local computer to the cluster, simply push your code from your local computer to GitHub, and then pull from GitHub to the cluster. This will make sure your code stays synchronized across these two computing environments.\n\n\nData\nThe easiest way to move data between your local computer and the cluster is also GitHub. However, this approach works only if the data files you are working with are under 100 MB, which is GitHub’s limit. Otherwise, the best approach is via rsync, a Linux utility for file synchronization.\n\n\n\nSubmitting batch jobs\n\nSGESLURM\n\n\nTo submit a batch job, you must first write a submission script. Here, I will illustrate how one would write a submission script for an array job. Suppose you would like to carry out a numerical simulation that compares 100 different parameter values (say 1, 2, …, 100). To do so, you would first create an R script called run_sim.R, which takes as a command-line argument the parameter value and writes the results of the corresponding simulation to file. Then, you would create a bash script called submit_jobs.sh, which specifies the resource requests as well as the code to run for each array task (the array task is specified by the environment variable SGE_TASK_ID).\n\n\nsubmit_jobs.sh\n\n#!/bin/bash\n\n#$ -N my_simulation             # Specify job name\n#$ -j y                         # Merge standard output and standard error\n#$ -l h_rt=01:00:00             # Request 1 hour of runtime\n#$ -l m_mem_free=1G             # Request 1 GB of virtual memory per slot\n#$ -t 1-100                     # Specify the task range for the array job\n\n# load requisite modules\nmodule load R/4.3.1\n\n# Calculate parameter for this task via scheduler's $SGE_TASK_ID\nparameter_value=$SGE_TASK_ID\n\n# Run the simulation with the specific parameter value\nRscript run_sim.R $parameter_value\n\nFinally, you would submit this array job to the scheduler via\nqsub submit_jobs.sh\nPlease see this GitHub repository for an example of using this paradigm to carry out a numerical simulation.\n\n\nTo submit a batch job, you must first write a submission script. Here, I will illustrate how one would write a submission script for an array job. Suppose you would like to carry out a numerical simulation that compares 100 different parameter values (say 1, 2, …, 100). To do so, you would first create an R script called run_sim.R, which takes as a command-line argument the parameter value and writes the results of the corresponding simulation to file. Then, you would create a bash script called submit_jobs.sh, which specifies the resource requests as well as the code to run for each array task (the array task is specified by the environment variable SLURM_ARRAY_TASK_ID.\n\n\nsubmit_jobs.sh\n\n#!/bin/bash\n\n#SBATCH --job-name=my_simulation    # Specify job name\n#SBATCH --time=01:00:00             # Request 1 hour of runtime\n#SBATCH --mem=1G                    # Request 1 GB of memory\n#SBATCH --array=1-100               # Specify the task range for the array job\n\n# load requisite modules\nmodule load R/4.3.1\n\n# Calculate parameter for this task via scheduler's $SLURM_ARRAY_TASK_ID\nparameter_value=$SLURM_ARRAY_TASK_ID\n\n# Run the simulation with the specific parameter value\nRscript run_sim.R $parameter_value\n\nFinally, you would submit this array job to the scheduler via\nsbatch submit_jobs.sh\n\n\n\n\n\nMonitoring the progress of batch jobs\nYou can monitor the progress of your batch jobs (or tasks, for array jobs) by viewing the status of queued or running jobs (via qstat on SGE or squeue on SLURM). These commands will show you how many of these jobs are still waiting in the queue and how many are running. If you wish to monitor the progress of each job (or task) individually, you can use its output file. These output files will usually be named something like &lt;job_name&gt;.o&lt;job_id&gt;.&lt;task_id&gt;, and will appear by default in the same directory as your submission script. If your code (in this case, run_sim.R) prints statements about its progress, then these will show up in the output file corresponding to each job and task.\n\n\nDebugging errors in batch jobs\nErrors in your batch jobs will typically be due to one of two reasons: a bug in your code or an inadequate resource request. The former issue is always present, but the latter is HPC-specific.\n\nBug in your code\nUsually, you would develop and debug your code locally prior to moving it to the cluster. This strategy should help reduce the number of bugs you encounter when running code on the cluster. If you do encounter a bug in your code on the cluster, then you can make note of the job and/or task id and try to reproduce and fix the bug in an interactive session on the cluster. Alternatively, you can go back to your local machine to fix the bug.\n\n\nInadequate resource request\nSuppose you requested an hour for a given batch job (or task) and that job actually needed two hours. In this case, the scheduler unceremoniously kills that job after one hour and prints an error message in the corresponding log file. In addition to inspecting the log file, you can get a better sense for what happened with a killed job via the job accounting feature of the scheduler to get a wealth of information about the given job/task:\n\nSGESLURM\n\n\nqacct -j &lt;job_id&gt; -t &lt;task_id&gt;\n\n\nsacct -j &lt;job_id&gt;_&lt;task_id&gt;\n\n\n\nTo deal with insufficient memory or time requests, simply restart the job with an increased memory or time request. You may need to run some pilot jobs to get a better sense for how much memory or time your code will need."
  },
  {
    "objectID": "figures.html",
    "href": "figures.html",
    "title": "13  Figures",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "14  Reports",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "manuscripts.html",
    "href": "manuscripts.html",
    "title": "15  Manuscripts",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "local-setup.html#installing-r",
    "href": "local-setup.html#installing-r",
    "title": "Appendix A — Local setup",
    "section": "A.1 Installing R",
    "text": "A.1 Installing R"
  },
  {
    "objectID": "local-setup.html#installing-rstudio",
    "href": "local-setup.html#installing-rstudio",
    "title": "Appendix A — Local setup",
    "section": "A.2 Installing RStudio",
    "text": "A.2 Installing RStudio"
  },
  {
    "objectID": "local-setup.html#installing-visual-studio-code",
    "href": "local-setup.html#installing-visual-studio-code",
    "title": "Appendix A — Local setup",
    "section": "A.3 Installing Visual Studio Code",
    "text": "A.3 Installing Visual Studio Code"
  },
  {
    "objectID": "local-setup.html#installing-wsl2-for-windows-users",
    "href": "local-setup.html#installing-wsl2-for-windows-users",
    "title": "Appendix A — Local setup",
    "section": "A.4 Installing WSL2 for Windows users",
    "text": "A.4 Installing WSL2 for Windows users"
  },
  {
    "objectID": "local-setup.html#sec-ssh-keygen",
    "href": "local-setup.html#sec-ssh-keygen",
    "title": "Appendix A — Local setup",
    "section": "A.5 Generate an SSH key",
    "text": "A.5 Generate an SSH key\nCheck whether you already have an SSH key by typing ls ~/.ssh at the command line and seeing whether are files named id_ed25519 and id_ed25519.pub. If not, generate an SSH key by typing\nssh-keygen -t ed25519 -C \"your_email@example.com\" \nat the command line. Press enter when prompted with Enter file in which to save the key to choose the default, and then type a passphrase when prompted with Enter passphrase (empty for no passphrase). Alternately, do not type a passphrase by simply pressing enter twice.\nIf you are on MacOS, create a file called config in the directory ~/.ssh with the contents\nHost *\n  AddKeysToAgent yes\n  UseKeychain yes\n  IdentityFile ~/.ssh/id_ed25519\nThe purpose of this step is to have the MacOS keychain store your SSH key passphrase."
  },
  {
    "objectID": "hpc-setup.html#sec-passwordless-hpc-access",
    "href": "hpc-setup.html#sec-passwordless-hpc-access",
    "title": "Appendix B — HPC setup",
    "section": "B.1 Set up password-less access to HPC",
    "text": "B.1 Set up password-less access to HPC\nFirst, generate an SSH key on your local machine. Then, copy your SSH public key from your local machine to the HPC by typing\nssh-copy-id &lt;username&gt;@&lt;hpc-address&gt;\nat the Terminal of your local machine and then enter your login credentials. At this point, you should be able to SSH into your HPC without entering your password; try it out by typing\nssh &lt;username&gt;@&lt;hpc-address&gt;"
  }
]