[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Computing and Communication",
    "section": "",
    "text": "Welcome\nThis is the website for my notes on Statistical Computing with R, which are currently a work in progress.\n\n\n\n\n\n\nWarning\n\n\n\nUnder construction."
  },
  {
    "objectID": "guiding-principles.html#reproducibility",
    "href": "guiding-principles.html#reproducibility",
    "title": "1  Guiding principles",
    "section": "1.1 Reproducibility",
    "text": "1.1 Reproducibility"
  },
  {
    "objectID": "guiding-principles.html#organization",
    "href": "guiding-principles.html#organization",
    "title": "1  Guiding principles",
    "section": "1.2 Organization",
    "text": "1.2 Organization"
  },
  {
    "objectID": "guiding-principles.html#automation",
    "href": "guiding-principles.html#automation",
    "title": "1  Guiding principles",
    "section": "1.3 Automation",
    "text": "1.3 Automation"
  },
  {
    "objectID": "guiding-principles.html#portability",
    "href": "guiding-principles.html#portability",
    "title": "1  Guiding principles",
    "section": "1.4 Portability",
    "text": "1.4 Portability"
  },
  {
    "objectID": "guiding-principles.html#synchronization",
    "href": "guiding-principles.html#synchronization",
    "title": "1  Guiding principles",
    "section": "1.5 Synchronization",
    "text": "1.5 Synchronization"
  },
  {
    "objectID": "guiding-principles.html#scalability",
    "href": "guiding-principles.html#scalability",
    "title": "1  Guiding principles",
    "section": "1.6 Scalability",
    "text": "1.6 Scalability"
  },
  {
    "objectID": "fundamental-skills.html",
    "href": "fundamental-skills.html",
    "title": "Fundamental Skills",
    "section": "",
    "text": "The following chapters cover a set of broadly useful fundamental skills."
  },
  {
    "objectID": "r-programming-basics.html",
    "href": "r-programming-basics.html",
    "title": "2  R programming",
    "section": "",
    "text": "See Chapters 2-8 of Advanced R."
  },
  {
    "objectID": "tidyverse.html",
    "href": "tidyverse.html",
    "title": "3  Tidyverse",
    "section": "",
    "text": "See Chapters 1-20 of R for Data Science."
  },
  {
    "objectID": "linux-basics.html",
    "href": "linux-basics.html",
    "title": "4  Linux basics",
    "section": "",
    "text": "For a first, brief introduction to Linux, read this page and then this page. For a somewhat more systematic introduction to the Linux shell, read Part I of The Linux Command Line: A Complete Introduction (Penn link). To build your Linux skills further, read Parts II and III as well."
  },
  {
    "objectID": "version-control.html",
    "href": "version-control.html",
    "title": "5  Version control",
    "section": "",
    "text": "Read Chapters 1 and 2 of Pro Git."
  },
  {
    "objectID": "directories.html",
    "href": "directories.html",
    "title": "6  Directories",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "real-data.html",
    "href": "real-data.html",
    "title": "7  Real data",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "r-programming.html",
    "href": "r-programming.html",
    "title": "R programming",
    "section": "",
    "text": "When programming, one often wishes to get to the answer as quickly as possible. However, programming quickly jeopardizes the correctness and reproducibility of the code that is written. The following chapters lay out a number of programming best practices to promote the correctness and reproducibility of code, which may take additional time to master at first but which save time in the long run. This document is geared towards R programming, though many of the principles presented here are language-agnostic.\nChapter 8 is for general suggestions, whereas Chapters 9 and 10 are specific to R scripting and R packages, respectively."
  },
  {
    "objectID": "programming-best-practices.html#code-readability",
    "href": "programming-best-practices.html#code-readability",
    "title": "8  Best practices",
    "section": "8.1 Code readability",
    "text": "8.1 Code readability\nCode readability refers to how easily a programmer can understand and interpret a piece of code. It is an essential aspect of software development because readable code is easier to maintain, faster to debug, more collaborative, and less prone to errors. Code readability can be broadly categorized into two main aspects: code style and code transparency. Code style pertains primarily to the format and presentation of the code. Code transparency delves deeper into the logic and structure of the code. It refers to how straightforwardly the functionality, logic, or operations are conveyed.\n\n8.1.1 Example\nConsider the following two examples:\nPoor code readability:\n\na = sum(dat[dat[,1]&gt;5 & dat[,2]&lt;10,3])\nb = mean(dat[dat[,1]&gt;5 & dat[,2]&lt;10,3])\nprint(paste('Sum =',a,', Mean =',b))\n\nGood code readability:\n\n# Library for data manipulation\nlibrary(dplyr)\n\n# Analysis parameters\nMIN_AGE &lt;- 5\nMAX_SCORE &lt;- 10\n\n# Extract summary performance metrics for subset of observations\nsummary_data &lt;- dat |&gt;\n  filter(age &gt;= MIN_AGE & score &lt;= MAX_SCORE) |&gt;\n  summarize(\n    sum_performance = sum(performance_metric),\n    mean_performance = mean(performance_metric)\n  )\n\n# Print the computed results\ncat(sprintf(\"Sum of Performance: %f | Mean of Performance: %f\", \n            summary_data$sum_performance, \n            summary_data$mean_performance))\n\nIn this section, we will discuss how to write code like that in the second snippet above.\n\n\n8.1.2 Code style\nCode should adhere to the tidyverse style guide, which includes guidance on the following aspects of code:\n\nNaming conventions\nSpacing and indentation\nCommenting\n\nYou can use the styler package to automatically conform your spacing and indentation to the tidyverse style guide."
  },
  {
    "objectID": "programming-best-practices.html#code-transparency",
    "href": "programming-best-practices.html#code-transparency",
    "title": "8  Best practices",
    "section": "8.2 Code transparency",
    "text": "8.2 Code transparency\nTo write transparent code, follow these guidelines:\n\nUse tidyverse paradigms as much as possible (e.g. dplyr summaries instead of apply operations)\nUse names, rather than indices, for subsetting (e.g. results[\"mse\", \"lasso\"] versus results[2,4])\nUse named arguments in function calls, especially with more than one argument (e.g. rbinom(3, 1, 0.5) versus rbinom(n = 3, size = 1, prob = 0.5))\nPut logically related chunks of code together into code blocks, with a comment describing the thrust of that code block.\nName constants in your scripts. “Magic numbers” are unexplained numbers in your scripts:\n\n# Bad practice: Magic number\nif (x &gt; 30) ...\n\n# Good practice: Using a named constant\nMAX_AGE &lt;- 30\nif (x &gt; MAX_AGE) ...\n\nDescriptive constant names provide clarity. Furthermore, especially if these constants are used in multiple places throughout your script, updating them becomes as simple as changing one line of code. It is also advisable to put all such constants together, near the top of the script."
  },
  {
    "objectID": "programming-best-practices.html#modularity",
    "href": "programming-best-practices.html#modularity",
    "title": "8  Best practices",
    "section": "8.3 Modularity",
    "text": "8.3 Modularity\nAvoid repetitive code. Repetition not only lengthens your script but also increases the chance for mistakes.\n\n# Bad practice: Repetitive code\ndata$age[data$age &lt; 0] &lt;- NA\ndata$score[data$score &lt; 0] &lt;- NA\n\n# Good practice: Create a function\nreplace_negatives_with_NA &lt;- function(variable) {\n  variable[variable &lt; 0] &lt;- NA\n  return(variable)\n}\n\ndata$age &lt;- replace_negatives_with_NA(data$age)\ndata$score &lt;- replace_negatives_with_NA(data$score)"
  },
  {
    "objectID": "programming-best-practices.html#code-speed",
    "href": "programming-best-practices.html#code-speed",
    "title": "8  Best practices",
    "section": "8.4 Code speed",
    "text": "8.4 Code speed\nWhen optimizing the execution speed of your code, it’s essential to strike a balance between code readability and efficiency. However, as Donald Knuth famously stated, ‘premature optimization is the root of all evil.’ Focus on writing clean and functional code first. Once your code works correctly, you can then consider optimizing the most computationally intensive parts if necessary.\n\n8.4.1 Vectorization\nR is a vectorized language, which means that operations can be performed on entire vectors rather than looping over individual elements. For example, instead of using a loop to square each element of a vector, you can simply square the vector directly:\n\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Non-vectorized operation using a loop\nsquared_numbers &lt;- vector(\"numeric\", length(numbers))\nfor (i in seq_along(numbers)) {\n  squared_numbers[i] &lt;- numbers[i]^2\n}\n\n# Example of vectorized operation\nsquared_numbers &lt;- numbers^2\n\n\n\n8.4.2 Factoring code out of loops\nOften, parts of the code inside a loop don’t depend on the loop variable and can be taken outside the loop, leading to efficiency gains. For example, if you’re repeatedly computing something within a loop that doesn’t change, compute it once outside the loop.\n\n# Inefficient loop: Compute mean of the entire dataset in each iteration\nfor (i in 1:n_bootstrap) {\n  sample &lt;- sample(data_points, size = length(data_points), replace = TRUE)\n  mu_data &lt;- mean(data_points) # This is unnecessary in the loop\n  bootstrap_means[i] &lt;- mean(sample) - mu_data\n}\n\n# Optimized loop: Factor out the mean computation of the dataset\nbootstrap_means_optimized &lt;- numeric(n_bootstrap)\nmu_data &lt;- mean(data_points)\nfor (i in 1:n_bootstrap) {\n  sample &lt;- sample(data_points, size = length(data_points), replace = TRUE)\n  bootstrap_means[i] &lt;- mean(sample) - mu_data\n}"
  },
  {
    "objectID": "r-scripting.html#managing-package-dependencies",
    "href": "r-scripting.html#managing-package-dependencies",
    "title": "9  Writing R scripts",
    "section": "9.1 Managing package dependencies",
    "text": "9.1 Managing package dependencies\nR’s rich ecosystem of packages is one of its strengths. However, with the increasing number of packages, managing dependencies becomes essential to ensure the reproducibility and reliability of your code. This section provides some best practices and recommended tools for handling package dependencies.\n\n9.1.1 Namespace conflicts\nWhen you load multiple packages, there is potential for functions with the same name to clash, leading to ambiguity in your code. This is because different packages can have functions that share the same name.\n\nUsing :: for explicit namespaces\nOne way to deal with such conflicts is by explicitly specifying the package’s namespace when calling the function using the :: syntax. This ensures that the correct function from the desired package is called. For example, if both packages A and B have a function named fun(), and you want to use the function from package A, you can do:\n\nA::fun()\n\n\n\nconflicted package\nAnother solution is to use the conflicted package. When loaded, conflicted will prevent you from using any function that has a namespace conflict, prompting you to specify which one you want:\n\nlibrary(conflicted)\nconflict_prefer(\"fun\", \"A\") # Always use fun from package A\n\n\n\n\n9.1.2 Managing package versions with renv\nFor the sake of reproducibility and easy collaboration, it is essential to track which versions of R packages were used for a given analysis. The renv package can help by creating isolated, reproducible R environments. Using renv, you can snapshot your current package versions, ensuring that others (or yourself in the future) can replicate your environment:\n\n# initializing renv\nrenv::init()\n\nBy doing this, renv will create a snapshot of your current package versions and save it in a renv.lock file. This file can then be shared, ensuring that the same package versions are used when your code is run elsewhere."
  },
  {
    "objectID": "r-scripting.html#code-portability",
    "href": "r-scripting.html#code-portability",
    "title": "9  Writing R scripts",
    "section": "9.2 Code portability",
    "text": "9.2 Code portability\nOne of the challenges in R programming, especially when sharing your code with others or transferring it between different machines, is ensuring code portability. Code portability ensures that your R script or project can be executed seamlessly in different environments without any hitches. Here’s how to enhance the portability of your R code:\n\n9.2.1 Avoid hardcoded paths\nHardcoding paths, such as C:/Users/JohnDoe/Documents/mydata.csv, can break the code when run on a different machine or if files are moved. Instead, always aim to use relative paths or dynamic paths that adjust based on the current directory. This ensures that as long as the directory structure remains consistent, the paths will always be correct.\n\n\n9.2.2 Use R Projects\nR Projects, a feature of RStudio, allow you to maintain a consistent working directory regardless of where the project is located on the machine. When you open an R Project, RStudio sets the working directory to the project’s root directory. This allows you to use relative paths effectively. To start an R Project, simply choose File &gt; New Project in RStudio."
  },
  {
    "objectID": "r-scripting.html#automation-and-reproducibility",
    "href": "r-scripting.html#automation-and-reproducibility",
    "title": "9  Writing R scripts",
    "section": "9.3 Automation and reproducibility",
    "text": "9.3 Automation and reproducibility\nTo ensure the robustness and reliability of your analyses, strive for automation and reproducibility. This approach ensures your work remains consistent and easily repeatable.\n\n9.3.1 Automate manual operations\nManual operations, including moving files, creating directories, and saving figures, are inefficient and error-prone. As many operations as possible should be automated instead.\n\n# Instead of manually downloading a file, use R to do it programmatically:\ndownload.file(url = \"https://example.com/data.csv\", destfile = \"data/data.csv\")\n\n\n# Instead of manually saving figures, use ggsave():\nggsave(\"plots/my_plot.png\", plot = p)\n\n\n\n9.3.2 Produce results by running entire scripts\nAll results should be produced by writing scripts and then executing those scripts in their entirety. While pieces of the script can be run manually during development, run the entire script on the data when the time comes to actually produce a result.\n\n\n9.3.3 Best practices\nLastly, always load required libraries at the top of your R scripts. This not only makes it clear which packages are necessary but also ensures that potential namespace conflicts are identified early on, leading to more predictable and stable code."
  },
  {
    "objectID": "r-packages.html#defensive-programming",
    "href": "r-packages.html#defensive-programming",
    "title": "10  Writing R packages",
    "section": "10.1 Defensive programming",
    "text": "10.1 Defensive programming\nWhen writing a function, the goal is for the function to either produce the intended output or an informative error message for any given input. Since a wide variety of inputs are possible, defensive programming is required to achieve this. Defensive programming in R entails anticipating potential issues, pitfalls, or mistakes in the code and implementing strategies to handle them gracefully.\nInput validation:\nOne of the most common tactics in defensive programming is input validation. Before processing, check if the provided inputs are valid or in the expected format. For instance, if a function expects a numeric vector, verify the input type before proceeding.\n\ncompute_mean &lt;- function(data_vector) {\n  if (!is.numeric(data_vector)) {\n    stop(\"The input data_vector must be numeric.\")\n  }\n  return(mean(data_vector))\n}\n\nUse informative error messages:\nWhen an error condition is detected, provide a clear and informative error message. This not only prevents silent failures but also helps users or developers identify and fix the problem.\n\nsafe_divide &lt;- function(numerator, denominator) {\n  if (denominator == 0) {\n    stop(\"Error: Division by zero is not allowed.\")\n  }\n  return(numerator / denominator)\n}"
  },
  {
    "objectID": "r-packages.html#further-reading",
    "href": "r-packages.html#further-reading",
    "title": "10  Writing R packages",
    "section": "10.2 Further reading",
    "text": "10.2 Further reading\nSee R Packages by Hadley Wickham and Jenny Bryan."
  },
  {
    "objectID": "hpc.html",
    "href": "hpc.html",
    "title": "High-performance computing",
    "section": "",
    "text": "High-performance computing is a computing paradigm that facilitates parallel computation across hundreds of computers. It is particularly effective for embarrassingly parallelizable computations, i.e. computations with many subparts that can be run independently of one another. Many institutions (academic and otherwise) have some sort of high-performance computing environment available for its members to use."
  },
  {
    "objectID": "hpc-basics.html#the-anatomy-of-an-hpc-cluster",
    "href": "hpc-basics.html#the-anatomy-of-an-hpc-cluster",
    "title": "11  HPC basics",
    "section": "11.1 The anatomy of an HPC cluster",
    "text": "11.1 The anatomy of an HPC cluster\n\n11.1.1 Hardware\nA high-performance computer cluster is a large collection of computers sharing a file system (Figure 11.1). Each computer is called a node, and these nodes are divided into two groups: login nodes and compute nodes. Login nodes are like a lobby area for the HPC; they are where you end up when you first enter. There are usually just a few login nodes. The vast majority of the nodes in an HPC are compute nodes; they are the workhorses of the HPC. Each compute node has some number of processors, or cores (e.g. 16) as well as some amount of RAM shared among those cores (e.g. 128 GB).\n\n\n\nFigure 11.1: High-performance computing setup (source)\n\n\n\n\n11.1.2 Software\nTypically, HPC clusters come with several version of a number of standard softwares pre-installed for all users. Users can build up their computing environment by loading modules corresponding to specific versions of specific softwares (e.g. R/4.3.1). The command module avail lists all available modules that can be loaded by the user, while module load &lt;module_name&gt; loads the specified module. The module system provides a convenient way to dynamically modify a user’s environment to include the necessary executables, libraries, and other resources required by a software package. This approach allows multiple versions of software to coexist on the cluster and enables users to switch between them as needed, facilitating reproducibility and flexibility in using various software tools and libraries on HPC clusters."
  },
  {
    "objectID": "hpc-basics.html#logging-onto-a-cluster",
    "href": "hpc-basics.html#logging-onto-a-cluster",
    "title": "11  HPC basics",
    "section": "11.2 Logging onto a cluster",
    "text": "11.2 Logging onto a cluster\nLogging onto a cluster can be done either via the Terminal or via a remote desktop. I will focus on the former approach. To log into a cluster via the Terminal, one typically uses ssh, e.g.\nssh ekatsevi@hpc3.wharton.upenn.edu\nUsually, you will be required to enter a password at this stage. However, I recommend setting up SSH keys in order to avoid entering your password each time you log onto the cluster. Upon gaining access, you will land in a login node."
  },
  {
    "objectID": "hpc-basics.html#two-modes-of-interacting-with-an-hpc-cluster",
    "href": "hpc-basics.html#two-modes-of-interacting-with-an-hpc-cluster",
    "title": "11  HPC basics",
    "section": "11.3 Two modes of interacting with an HPC cluster",
    "text": "11.3 Two modes of interacting with an HPC cluster\nThere are two modes of interacting with an HPC cluster: interactive jobs and batch jobs.\n\nAn interactive job entails directly entering a compute node and running computer programs from there. (It may be tempting to start computing directly in a login node, but this is strongly discouraged.) Interactive jobs are useful for initial exploration and debugging but not for more serious computations. Indeed, interactive jobs do not give you access to multiple compute nodes and whatever computations you start will be interrupted if you lose connection with the cluster.\nA batch job is a computing job that you “send to the cluster” for completion.The power of batch jobs is that you can submit many at a time in order to take advantage of the large number of computers in the cluster. Furthermore, once you submit a batch job, you need not be logged into the cluster for the job to finish running. Typically, batch jobs are submitted en masse via a submission script."
  },
  {
    "objectID": "hpc-basics.html#sec-using-scheduler",
    "href": "hpc-basics.html#sec-using-scheduler",
    "title": "11  HPC basics",
    "section": "11.4 Requesting resources via the scheduler",
    "text": "11.4 Requesting resources via the scheduler\nWhether you would like to run an interactive job or a batch job, you need to request resources (the number of cores, the amount of computing time, and the amount of memory) from the scheduler, a software running on the HPC that allocates shared resources among its users. Below is an example of how a scheduler may allocate resources to several jobs.\n\n\n\nScheduling multiple jobs with different resource requirements (source)\n\n\nJobs can be scheduled based on various algorithms, which take into account their resource requests and their order in which they are submitted. Any submitted job lands in the queue, where it waits until there are available resources to run. Many HPC clusters have multiple queues for different kinds of jobs. For example, there may be separate queues for jobs with particularly short or long requested runtimes, or for jobs with particularly small or large requested memory allocations.\n\n\n\nEach submitted job lands in the queue prior to execution (source)\n\n\nThere are several different schedulers in use by HPC clusters, two of the most common being Sun Grid Engine (SGE) and Simple Linux Utility for Resource Management (SLURM). These different schedulers have different syntax for tasks like submitting jobs, viewing the job queue, etc.\n\n\n\n\n\n\n\n\nTask\nSGE Command\nSLURM Command\n\n\n\n\nSubmit interactive job\nqrsh\nsrun\n\n\nSubmit batch job\nqsub &lt;script&gt;\nsbatch &lt;script&gt;\n\n\nResource request flags\n-l h_rt=01:00:00 -l h_vmem=1G\n--time=01:00:00 --mem=1G\n\n\nDelete a job\nqdel &lt;job_id&gt;\nscancel &lt;job_id&gt;\n\n\nView job status\nqstat\nsqueue\n\n\nJob accounting\nqacct\nsacct"
  },
  {
    "objectID": "hpc-basics.html#example-batch-submission-script",
    "href": "hpc-basics.html#example-batch-submission-script",
    "title": "11  HPC basics",
    "section": "11.5 Example batch submission script",
    "text": "11.5 Example batch submission script\nTo illustrate more concretely how one might submit multiple batch jobs to the scheduler, suppose you would like to carry out a numerical simulation that compares 100 different parameter values (say 1, 2, …, 100). To do so, you would first create an R script called run_sim.R, which takes as a command-line argument the parameter value and writes the results of the corresponding simulation to file. Then, you would create a bash script called run_sim.sh, whose job is to set up the computing environment and then call run_sim.R with the given parameter:\n\n\nrun_sim.sh\n\n#!/bin/bash\n\n# read command-line argument\nparam=$1\n\n# load requisite modules\nmodule load R/4.3.1\n\n# call run_sim.R with specified parameter\nRscript run_sim.R $param\n\nThen, you would create a bash script called submit_jobs.sh, which submits a special kind of job called an array job. Technically this is just one job, but it consists of many tasks, one for each simulation setting (specified by the environment variable SGE_TASK_ID provided by the scheduler, in the case of SGE). This is conceptually similar to, but more efficient than looping over all the parameter values and submitting a batch job for each.\n\n\nsubmit_jobs.sh\n\n#!/bin/bash\n\n#$ -N my_simulation             # Specify job name\n#$ -j y                         # Merge standard output and standard error\n#$ -l h_rt=01:00:00             # Request 1 hour of runtime\n#$ -l h_vmem=1G                 # Request 1 GB of virtual memory per slot\n#$ -t 1-100                     # Specify the task range for the array job\n\n# Calculate parameter setting for this task via scheduler's $SGE_TASK_ID\nparameter_value=$SGE_TASK_ID\n\n# Run the simulation with the specific parameter value\nbash run_sim.sh $parameter_value\n\nFinally, we would submit this array job to the scheduler (in this case, SGE) via\nqsub submit_jobs.sh"
  },
  {
    "objectID": "hpc-basics.html#monitoring-the-progress-of-batch-jobs",
    "href": "hpc-basics.html#monitoring-the-progress-of-batch-jobs",
    "title": "11  HPC basics",
    "section": "11.6 Monitoring the progress of batch jobs",
    "text": "11.6 Monitoring the progress of batch jobs\nYou can monitor the progress of your batch jobs (or tasks, for array jobs) by viewing the status of queued or running jobs (via qstat on SGE or squeue on SLURM). These commands will show you how many of these jobs are still waiting in the queue and how many are running. If you wish to monitor the progress of each job (or task) individually, you can use its output file. These output files will usually be named something like &lt;job_name&gt;.o&lt;job_id&gt;.&lt;task_id&gt;, and will appear by default in the same directory as your submission script. If your code (in this case, run_sim.R) prints statements about its progress, then these will show up in the output file corresponding to each job and task."
  },
  {
    "objectID": "hpc-basics.html#debugging-errors-in-batch-jobs",
    "href": "hpc-basics.html#debugging-errors-in-batch-jobs",
    "title": "11  HPC basics",
    "section": "11.7 Debugging errors in batch jobs",
    "text": "11.7 Debugging errors in batch jobs\nErrors in your batch jobs will typically be due to one of two reasons: a bug in your code or an inadequate resource request. The former issue is always present, but the latter is HPC-specific.\n\n11.7.1 Bug in your code\nUsually, you would develop and debug your code locally prior to moving it to the cluster. This strategy should help reduce the number of bugs you encounter when running code on the cluster. If you do encounter a bug in your code on the cluster, then you can make note of the job and/or task id and try to reproduce and fix the bug in an interactive session on the cluster. Alternatively, you can go back to your local machine to fix the bug.\n\n\n11.7.2 Inadequate resource request\nSuppose you requested an hour for a given batch job (or task) and that job actually needed two hours. In this case, the scheduler unceremoniously kills that job after one hour and prints an error message in the corresponding log file. In addition to inspecting the log file, you can get a better sense for what happened with a killed job via the job accounting feature of the scheduler. For example, on SGE you can type\nqacct -j &lt;job_id&gt; -t &lt;task_id&gt;\nto get a wealth of information about the given job/task. To deal with insufficient memory or time requests, simply restart the job with an increased memory or time request. You may need to run some pilot jobs to get a better sense for how much memory or time your code will need."
  },
  {
    "objectID": "hpc-basics.html#merging-results-and-exporting-them-to-your-computer",
    "href": "hpc-basics.html#merging-results-and-exporting-them-to-your-computer",
    "title": "11  HPC basics",
    "section": "11.8 Merging results and exporting them to your computer",
    "text": "11.8 Merging results and exporting them to your computer"
  },
  {
    "objectID": "figures.html",
    "href": "figures.html",
    "title": "13  Figures",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "14  Reports",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "manuscripts.html",
    "href": "manuscripts.html",
    "title": "15  Manuscripts",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction."
  },
  {
    "objectID": "local-setup.html#installing-r",
    "href": "local-setup.html#installing-r",
    "title": "Appendix A — Local setup",
    "section": "A.1 Installing R",
    "text": "A.1 Installing R"
  },
  {
    "objectID": "local-setup.html#installing-rstudio",
    "href": "local-setup.html#installing-rstudio",
    "title": "Appendix A — Local setup",
    "section": "A.2 Installing RStudio",
    "text": "A.2 Installing RStudio"
  },
  {
    "objectID": "local-setup.html#installing-visual-studio-code",
    "href": "local-setup.html#installing-visual-studio-code",
    "title": "Appendix A — Local setup",
    "section": "A.3 Installing Visual Studio Code",
    "text": "A.3 Installing Visual Studio Code"
  },
  {
    "objectID": "local-setup.html#installing-wsl2-for-windows-users",
    "href": "local-setup.html#installing-wsl2-for-windows-users",
    "title": "Appendix A — Local setup",
    "section": "A.4 Installing WSL2 for Windows users",
    "text": "A.4 Installing WSL2 for Windows users"
  },
  {
    "objectID": "local-setup.html#sec-ssh-keygen",
    "href": "local-setup.html#sec-ssh-keygen",
    "title": "Appendix A — Local setup",
    "section": "A.5 Generate an SSH key",
    "text": "A.5 Generate an SSH key\nCheck whether you already have an SSH key by typing ls ~/.ssh at the command line and seeing whether are files named id_ed25519 and id_ed25519.pub. If not, generate an SSH key by typing\nssh-keygen -t ed25519 -C \"your_email@example.com\" \nat the command line. Press enter when prompted with Enter file in which to save the key to choose the default, and then type a passphrase when prompted with Enter passphrase (empty for no passphrase). Alternately, do not type a passphrase by simply pressing enter twice.\nIf you are on MacOS, create a file called config in the directory ~/.ssh with the contents\nHost *\n  AddKeysToAgent yes\n  UseKeychain yes\n  IdentityFile ~/.ssh/id_ed25519\nThe purpose of this step is to have the MacOS keychain store your SSH key passphrase."
  },
  {
    "objectID": "hpc-setup.html#sec-passwordless-hpc-access",
    "href": "hpc-setup.html#sec-passwordless-hpc-access",
    "title": "Appendix B — HPC setup",
    "section": "B.1 Set up password-less access to HPC",
    "text": "B.1 Set up password-less access to HPC\nFirst, generate an SSH key on your local machine. Then, copy your SSH public key from your local machine to the HPC by typing\nssh-copy-id &lt;username&gt;@&lt;hpc-address&gt;\nat the Terminal of your local machine and then enter your login credentials. At this point, you should be able to SSH into your HPC without entering your password; try it out by typing\nssh &lt;username&gt;@&lt;hpc-address&gt;"
  }
]